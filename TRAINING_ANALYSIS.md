# Training Analysis Report
## ASL Translation Framework - Initial Training Run

**Date**: January 5, 2026
**Model**: BiLSTM Encoder + CTC Head
**Dataset**: WLASL (20 train, 10 val samples)
**Best Checkpoint**: Epoch 3 (val_loss=10.07)

---

## üìä Training Metrics

### Loss Curves
![Training Curves](checkpoints/real_mediapipe_cached/training_curves.png)

### Statistics
- **Epochs**: 10
- **Train Loss**: 724.98 ‚Üí 2.56 (99.6% reduction) ‚úì
- **Val Loss**: 367.14 ‚Üí 10.07 (best at epoch 3)
- **Overfitting**: Yes - val loss plateaued after epoch 3 ‚ö†Ô∏è

### Model Architecture
- **Parameters**: 1,178,961 (1.18M)
- **Encoder**: BiLSTM (128 hidden √ó 2 layers √ó bidirectional)
- **Vocabulary**: 2001 (2000 glosses + blank)
- **Input**: 36D phonological features

---

## üîç Diagnosis: BLANK COLLAPSE

### Problem
The model **always predicts blank (ID:0)** with 98.5-98.7% confidence.

### Evidence
```
Train Accuracy: 0%
Val Accuracy: 0%

Example predictions:
‚úó Video 69241 (book):
   Predicted: <BLANK> (98.5%)
   True label: book (0.2%)

‚úó Video 69302 (drink):
   Predicted: <BLANK> (98.7%)
   True label: drink (0.0%)
```

### Root Causes

1. **Massive Parameter/Data Imbalance**
   - 1.18M parameters vs 20 training samples
   - Ratio: **59,000:1** (parameters per sample!)
   - Typical ratio for good generalization: <1000:1

2. **Vocabulary Too Large**
   - 2001 classes (2000 glosses + blank)
   - Only 2 unique classes in training data (book, drink)
   - Model can't learn meaningful distinctions

3. **CTC Blank Bias**
   - CTC loss allows trivial solution: always predict blank
   - Without proper regularization, model takes shortcut
   - Training loss goes down (memorization) but predictions collapse

4. **Insufficient Data Diversity**
   - Only 20 videos, likely from same 2-3 signs
   - No variation to learn generalizable patterns
   - Model memorizes noise instead of signal

---

## üìà What Actually Worked

Despite 0% accuracy, some things DID work:

‚úÖ **Infrastructure**:
- Real MediaPipe integration working
- Feature extraction pipeline functional
- GPU training stable (56¬∞C)
- Feature caching fast (320x speedup)
- Type-safe codebase (0 errors after fixes)

‚úÖ **Training Loop**:
- CTC loss computed correctly (no NaN/inf)
- Gradients flowing (train loss decreased)
- Checkpointing working
- Validation running

‚úÖ **Technical Stack**:
- PyTorch + CUDA 12.8
- MediaPipe 0.10+ Tasks API
- BiLSTM encoder architecture sound

---

## üéØ Recommended Fixes

### Immediate (Priority 1)

**1. Reduce Model Size** (Target: <100K params)
```python
EncoderConfig(
    hidden_dim=64,      # Was: 128
    num_layers=1,       # Was: 2
    bidirectional=True,  # Keep
)
```
- Expected params: ~50K (20x reduction)
- Better params/data ratio: 2500:1

**2. Filter Vocabulary** (Use only seen classes)
```python
# Instead of all 2001 glosses, use only:
vocab = unique_glosses_in_train_set + [blank]
# For current data: vocab_size = 3 (book, drink, blank)
```

**3. Increase Training Data** (Minimum: 100 samples/class)
```python
max_samples_train = 500   # Was: 20
max_samples_val = 100     # Was: 10
```
- Aim for 10-20 different sign classes
- At least 20-50 examples per class

**4. Add CTC Blank Penalty** (Discourage blank predictions)
```python
# In CTCLoss forward():
blank_penalty = 0.1  # Penalize blank predictions
loss = ctc_loss(...) + blank_penalty * blank_ratio
```

### Medium-Term (Priority 2)

**5. Curriculum Learning**
- Start with 2-class problem (book vs drink)
- Add classes gradually
- Verify >80% accuracy before scaling

**6. Data Augmentation**
- Temporal: frame dropping, speed changes
- Spatial: small rotations (preserve Sim(3) invariance)
- Noise injection in feature space

**7. Better Regularization**
```python
EncoderConfig(
    dropout=0.5,        # Was: 0.3
)
# Add L2 weight decay
optimizer = Adam(..., weight_decay=1e-4)
```

**8. Learning Rate Tuning**
- Try smaller LR: 1e-4 (was 1e-3)
- Add LR scheduler (reduce on plateau)
- Monitor gradient norms

### Long-Term (Priority 3)

**9. Attention Mechanism**
- Replace/augment BiLSTM with attention
- Better temporal modeling
- Fewer parameters

**10. Pre-training**
- Self-supervised on unlabeled videos
- Learn phonological features better
- Transfer to supervised task

**11. Multi-Task Learning**
- Joint training on phonology + gloss
- Auxiliary losses for segmentation
- Prevents blank collapse

---

## üöÄ Recommended Next Steps

### Option A: Quick Fix (1-2 hours)
1. Reduce model to 50K params (hidden=64, layers=1)
2. Train on 500 samples from 10 different signs
3. Add blank penalty to loss
4. Verify >50% accuracy before scaling

### Option B: Systematic Rebuild (1-2 days)
1. Implement full data loading (500-1000 samples)
2. Reduce vocab to seen classes only
3. Smaller model + better regularization
4. Curriculum: 2-class ‚Üí 10-class ‚Üí 100-class ‚Üí 2000-class
5. Track metrics properly (top-k accuracy, per-class)

### Option C: Research Direction (1-2 weeks)
1. Implement self-supervised pre-training (Stage 1)
2. Learn phonological quantizer first
3. Then train gloss classifier
4. Follow paper's 3-stage curriculum

---

## üìù Lessons Learned

1. **More data > bigger model** - Always
2. **Start simple, scale gradually** - 2-class first
3. **Monitor predictions, not just loss** - Blank collapse invisible in loss curves
4. **Regularize aggressively** - CTC needs it
5. **Validate infrastructure before scaling** - We did this right! ‚úì

---

## üìä Comparison to Baselines

| Metric | Our Model | Random Baseline | Reasonable Target |
|--------|-----------|-----------------|-------------------|
| Train Acc | 0% | 0.05% (1/2000) | >80% |
| Val Acc | 0% | 0.05% | >50% |
| Params | 1.18M | - | <100K |
| Data | 20 | - | >500 |
| Vocab | 2001 | - | 10-50 initially |

**Verdict**: Model is **worse than random** due to blank collapse. Need fundamental fixes.

---

## üéì Mathematical Validation

Despite blank collapse, we DID validate some theory:

‚úÖ **Proposition 1 (Sim(3) Invariance)**:
- Features are geometrically normalized
- MediaPipe landmarks properly transformed
- Feature extractor implements equations 1-6 correctly

‚úÖ **CTC Soundness**:
- Loss computed correctly (matches PyTorch CTCLoss)
- Decoding (greedy) works
- Problem is model capacity/data, not CTC itself

‚ö†Ô∏è **Theorem 2 (Convergence)**:
- Training loss converged (99.6% reduction) ‚úì
- But to degenerate solution (blank collapse) ‚úó
- Need better initialization/regularization

---

## üîß Code Quality

After diagnostic fixes:
- **VS Code diagnostics**: 37 ‚Üí 10 (0 errors, 0 warnings)
- **Type safety**: 100% (all Pylance errors resolved)
- **Code cleanliness**: All unused imports/vars removed
- **Infrastructure**: Analysis tools working

---

## üìö References

- **Blank Collapse**: Graves et al. (2006) - CTC: Labelling Unsegmented Sequence Data
- **Data Efficiency**: "You need at least 100 examples per class for deep learning" (empirical rule)
- **Model Size**: "Parameters should be < 1000x training samples" (generalization bound)

---

## ‚úÖ Action Items

- [ ] Reduce model size to 50K params
- [ ] Load 500+ training samples (10 signs √ó 50 examples)
- [ ] Filter vocab to seen classes
- [ ] Add blank penalty to CTC loss
- [ ] Train and verify >50% accuracy
- [ ] If successful, scale to 100 classes
- [ ] Document training curves at each stage

---

**Status**: Ready to proceed with Option A (Quick Fix) or Option B (Systematic Rebuild).
