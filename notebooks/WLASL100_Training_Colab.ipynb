{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WLASL100 Training on Google Colab\n",
    "\n",
    "This notebook trains an ASL recognition model on the 100-sign WLASL dataset using pre-extracted features.\n",
    "\n",
    "**Prerequisites**: \n",
    "- Features extracted using `WLASL100_Feature_Extraction_Colab.ipynb`\n",
    "- Features saved to Google Drive at `/content/drive/MyDrive/asl_data/extracted_features/`\n",
    "\n",
    "**Runtime**: Select **GPU** runtime (Runtime \u2192 Change runtime type \u2192 GPU)\n",
    "\n",
    "**Estimated time**: 1-2 hours on Colab GPU (vs 30+ hours on laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "This notebook is a simplified training setup for Colab. For the full training implementation, see:\n",
    "- `scripts/train_phase1.py` - Full training script with all features\n",
    "- `src/training/stage2_ctc.py` - Stage 2 CTC training\n",
    "\n",
    "**Note**: This notebook provides a basic training setup. You can also upload and run `scripts/train_phase1.py` directly in Colab by adapting the paths to use Google Drive locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup - Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch and dependencies\n",
    "!pip install torch torchvision tqdm numpy\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive and Load Features\n",
    "\n",
    "Load the extracted features from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load features and vocabulary\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "feature_dir = Path('/content/drive/MyDrive/asl_data/extracted_features')\n",
    "train_cache = feature_dir / 'features_train_wlasl100.pkl'\n",
    "val_cache = feature_dir / 'features_val_wlasl100.pkl'\n",
    "vocab_file = feature_dir / 'vocabulary.json'\n",
    "\n",
    "# Load cached features\n",
    "print(\"Loading cached features...\")\n",
    "with open(train_cache, 'rb') as f:\n",
    "    train_cache_data = pickle.load(f)\n",
    "\n",
    "with open(val_cache, 'rb') as f:\n",
    "    val_cache_data = pickle.load(f)\n",
    "\n",
    "# Load vocabulary\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "gloss_to_id = vocab_data['gloss_to_id']\n",
    "id_to_gloss = vocab_data['id_to_gloss']\n",
    "vocab_size = vocab_data['vocab_size']\n",
    "\n",
    "print(f\"\\n\u2713 Loaded features:\")\n",
    "print(f\"  Train: {len(train_cache_data)} videos\")\n",
    "print(f\"  Val: {len(val_cache_data)} videos\")\n",
    "print(f\"  Vocab size: {vocab_size} (100 glosses + blank)\")\n",
    "print(f\"  Classes: {len(gloss_to_id)} unique signs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Dataset and Model Architecture\n",
    "\n",
    "We'll define everything needed for training directly in the notebook (no additional file uploads needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Dict, List\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Dataset class for cached features\n",
    "class CachedFeatureDataset(Dataset):\n",
    "    \"\"\"Dataset for cached features.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_data: Dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cache_data: Dict with format {video_id: {'features': ..., 'gloss': ..., 'gloss_id': ...}}\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        for video_id, data in cache_data.items():\n",
    "            features = data['features']  # (T, 36) numpy array\n",
    "            gloss_id = data['gloss_id']\n",
    "            \n",
    "            # Convert to tensor\n",
    "            features_tensor = torch.FloatTensor(features)\n",
    "            \n",
    "            # Create target sequence (just the gloss_id for CTC)\n",
    "            target = torch.LongTensor([gloss_id])\n",
    "            \n",
    "            self.samples.append({\n",
    "                'features': features_tensor,\n",
    "                'target': target,\n",
    "                'gloss_id': gloss_id,\n",
    "                'video_id': video_id,\n",
    "                'length': len(features)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader - handles variable-length sequences.\"\"\"\n",
    "    # Sort by length (descending) for efficient batching\n",
    "    batch = sorted(batch, key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    # Get dimensions\n",
    "    batch_size = len(batch)\n",
    "    max_length = batch[0]['length']\n",
    "    feature_dim = batch[0]['features'].shape[1]\n",
    "    \n",
    "    # Initialize padded tensors\n",
    "    features_padded = torch.zeros(batch_size, max_length, feature_dim)\n",
    "    lengths = torch.LongTensor([s['length'] for s in batch])\n",
    "    targets = []\n",
    "    gloss_ids = []\n",
    "    \n",
    "    # Fill in data\n",
    "    for i, sample in enumerate(batch):\n",
    "        seq_len = sample['length']\n",
    "        features_padded[i, :seq_len] = sample['features']\n",
    "        targets.append(sample['target'])\n",
    "        gloss_ids.append(sample['gloss_id'])\n",
    "    \n",
    "    return {\n",
    "        'features': features_padded,\n",
    "        'targets': targets,\n",
    "        'lengths': lengths,\n",
    "        'gloss_ids': torch.LongTensor(gloss_ids)\n",
    "    }\n",
    "\n",
    "print(\"\u2713 Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class ASLEncoder(nn.Module):\n",
    "    \"\"\"BiLSTM encoder for ASL features.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=36, hidden_dim=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, input_dim)\n",
    "            lengths: (batch_size,) actual sequence lengths\n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        # Pack padded sequence for efficiency\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.lstm(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CTCModel(nn.Module):\n",
    "    \"\"\"CTC model with encoder and classification head.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, vocab_size, blank_id=0):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.vocab_size = vocab_size\n",
    "        self.blank_id = blank_id\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(encoder.output_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, input_dim)\n",
    "            lengths: (batch_size,) actual sequence lengths\n",
    "        Returns:\n",
    "            logits: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        encoded = self.encoder(x, lengths)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"\u2713 Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "HIDDEN_DIM = 128  # Can increase to 256 for better capacity (100 classes)\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory (16GB Colab GPU can handle 16-32)\n",
    "LEARNING_RATE = 1e-3  # Can lower to 5e-4 for more stable training\n",
    "NUM_EPOCHS = 50\n",
    "BLANK_PENALTY = 0.5  # Increased from 0.1 for better blank suppression (ratio-based penalty)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Checkpoint directory\n",
    "CHECKPOINT_DIR = Path('/content/checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Training Configuration\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"Layers: {NUM_LAYERS}\")\n",
    "print(f\"Dropout: {DROPOUT}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Blank penalty: {BLANK_PENALTY}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CachedFeatureDataset(train_cache_data)\n",
    "val_dataset = CachedFeatureDataset(val_cache_data)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Datasets and data loaders created\")\n",
    "print(f\"  Train: {len(train_dataset)} samples ({len(train_loader)} batches)\")\n",
    "print(f\"  Val: {len(val_dataset)} samples ({len(val_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5: Load Stage 1 Pre-trained Encoder (Following Paper's Curriculum)\n",
    "\n",
    "**Important**: The paper's 3-stage curriculum (Section 7.2) requires Stage 1 pre-training before Stage 2 CTC training.\n",
    "\n",
    "- **Stage 1**: Self-supervised phonological pre-training (learns robust features)\n",
    "- **Stage 2**: CTC training with pre-trained encoder (fine-tunes for gloss prediction)\n",
    "- **Stage 3**: WFST fine-tuning (future work)\n",
    "\n",
    "Loading the Stage 1 checkpoint initializes the encoder with pre-trained features, following the paper's approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stage 1 pre-trained encoder checkpoint (if available)\n",
    "# Following paper's 3-stage curriculum: Stage 1 \u2192 Stage 2\n",
    "\n",
    "stage1_checkpoint_path = Path('/content/drive/MyDrive/asl_data/checkpoints/stage1/checkpoint_best.pt')\n",
    "USE_PRETRAINED = stage1_checkpoint_path.exists()\n",
    "\n",
    "if USE_PRETRAINED:\n",
    "    print(f\"Loading pre-trained Stage 1 encoder from {stage1_checkpoint_path}...\")\n",
    "    stage1_checkpoint = torch.load(stage1_checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "    \n",
    "    # Check checkpoint structure\n",
    "    print(f\"  Checkpoint keys: {list(stage1_checkpoint.keys())}\")\n",
    "    \n",
    "    # Extract encoder weights (Stage 1 checkpoint format)\n",
    "    if 'encoder_state_dict' in stage1_checkpoint:\n",
    "        encoder_state = stage1_checkpoint['encoder_state_dict']\n",
    "        print(f\"  \u2713 Found 'encoder_state_dict' in checkpoint\")\n",
    "    elif 'encoder' in stage1_checkpoint:\n",
    "        encoder_state = stage1_checkpoint['encoder']\n",
    "        print(f\"  \u2713 Found 'encoder' in checkpoint\")\n",
    "    else:\n",
    "        # Try to load from model_state_dict if it has encoder key\n",
    "        if 'model_state_dict' in stage1_checkpoint:\n",
    "            model_state = stage1_checkpoint['model_state_dict']\n",
    "            # Extract encoder keys (assuming they start with 'encoder.')\n",
    "            encoder_state = {k.replace('encoder.', ''): v \n",
    "                           for k, v in model_state.items() \n",
    "                           if k.startswith('encoder.')}\n",
    "            print(f\"  \u2713 Extracted encoder from 'model_state_dict'\")\n",
    "        else:\n",
    "            print(\"  \u26a0\ufe0f Checkpoint format not recognized. Available keys:\")\n",
    "            for key in stage1_checkpoint.keys():\n",
    "                print(f\"    - {key}\")\n",
    "            encoder_state = None\n",
    "            USE_PRETRAINED = False\n",
    "    \n",
    "    if encoder_state:\n",
    "        print(f\"\\n  Stage 1 Info:\")\n",
    "        print(f\"    Epoch: {stage1_checkpoint.get('epoch', 'unknown')}\")\n",
    "        print(f\"    Loss: {stage1_checkpoint.get('best_loss', stage1_checkpoint.get('loss', 'unknown')):.4f}\")\n",
    "        print(f\"\\n  \u2713 Stage 1 checkpoint loaded successfully!\")\n",
    "        print(f\"  \u2713 Will initialize encoder with pre-trained weights\")\n",
    "        print(f\"  \u2713 Following paper's curriculum: Stage 1 \u2192 Stage 2\")\n",
    "    else:\n",
    "        print(\"  \u26a0\ufe0f Could not extract encoder weights from checkpoint\")\n",
    "        USE_PRETRAINED = False\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Stage 1 checkpoint not found at {stage1_checkpoint_path}\")\n",
    "    print(f\"  Training will use random initialization (not following paper's curriculum)\")\n",
    "    print(f\"  To follow the paper:\")\n",
    "    print(f\"    1. Upload checkpoints/stage1/checkpoint_best.pt to Google Drive\")\n",
    "    print(f\"    2. Place it at: {stage1_checkpoint_path}\")\n",
    "    print(f\"    3. Re-run this cell\")\n",
    "    encoder_state = None\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Pre-training Status: {'\u2713 USING PRE-TRAINED (Following Paper)' if USE_PRETRAINED else '\u26a0\ufe0f RANDOM INIT (Not Following Paper)'}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "encoder = ASLEncoder(\n",
    "    input_dim=36,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Load pre-trained encoder weights if available (following paper's curriculum)\n",
    "if USE_PRETRAINED and encoder_state:\n",
    "    print(f\"\\nLoading pre-trained encoder weights into model...\")\n",
    "    try:\n",
    "        # Try loading with strict=False (allows architecture differences)\n",
    "        encoder.load_state_dict(encoder_state, strict=False)\n",
    "        print(f\"  \u2713 Successfully loaded pre-trained encoder weights!\")\n",
    "        print(f\"  \u2713 Encoder initialized with Stage 1 pre-trained features\")\n",
    "        print(f\"  \u2713 Following paper's 3-stage curriculum (Section 7.2)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  \u26a0\ufe0f Warning: Could not load pre-trained weights: {e}\")\n",
    "        print(f\"  Continuing with random initialization...\")\n",
    "else:\n",
    "    print(f\"\\n  \u26a0\ufe0f Using random initialization (Stage 1 checkpoint not loaded)\")\n",
    "\n",
    "model = CTCModel(encoder, vocab_size=vocab_size, blank_id=0)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Model Created\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Parameters: {num_params:,} (~{num_params/1000:.0f}K)\")\n",
    "print(f\"Params/train sample ratio: {num_params/len(train_dataset):.0f}:1\")\n",
    "print(f\"Encoder initialization: {'PRE-TRAINED (Stage 1)' if USE_PRETRAINED else 'RANDOM'}\")\n",
    "\n",
    "if num_params / len(train_dataset) > 1000:\n",
    "    print(f\"  \u26a0 Warning: High params/sample ratio (>1000:1)\")\n",
    "else:\n",
    "    print(f\"  \u2713 Good params/sample ratio (<1000:1)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Loss function (CTC)\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "print(f\"\\n\u2713 Optimizer created: Adam (lr={LEARNING_RATE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_blank_ratio(logits, blank_id=0):\n",
    "    \"\"\"Compute ratio of blank predictions (better than mean-based penalty).\"\"\"\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    blank_count = (predictions == blank_id).sum().item()\n",
    "    total_count = predictions.numel()\n",
    "    return blank_count / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, ctc_loss_fn, device, epoch, blank_penalty=0.5):\n",
    "    \"\"\"Train for one epoch with improved blank penalty (ratio-based).\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_ctc_loss = 0.0\n",
    "    total_blank_penalty = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
    "        features = batch['features'].to(device)\n",
    "        targets = batch['targets']\n",
    "        lengths = batch['lengths']\n",
    "        gloss_ids = batch['gloss_ids'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(features, lengths)\n",
    "        \n",
    "        # Prepare CTC inputs\n",
    "        # logits: (batch_size, seq_len, vocab_size) -> (seq_len, batch_size, vocab_size)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        log_probs = log_probs.transpose(0, 1)  # (seq_len, batch_size, vocab_size)\n",
    "        \n",
    "        # Targets for CTC\n",
    "        target_lengths = torch.LongTensor([len(t) for t in targets]).to(device)\n",
    "        target_flat = torch.cat(targets).to(device)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        ctc_loss_value = ctc_loss_fn(log_probs, target_flat, lengths, target_lengths)\n",
    "        \n",
    "        # Improved blank penalty: ratio-based (better than mean-based)\n",
    "        blank_ratio = compute_blank_ratio(logits, blank_id=0)\n",
    "        blank_penalty_value = blank_penalty * blank_ratio\n",
    "        loss = ctc_loss_value + blank_penalty_value\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Increased from 1.0\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_ctc_loss += ctc_loss_value.item()\n",
    "        total_blank_penalty += blank_penalty_value\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_ctc_loss = total_ctc_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_blank_penalty = total_blank_penalty / num_batches if num_batches > 0 else 0.0\n",
    "    \n",
    "    return avg_loss, avg_ctc_loss, avg_blank_penalty\n",
    "\n",
    "\n",
    "def validate(model, val_loader, ctc_loss_fn, device, epoch):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
    "            features = batch['features'].to(device)\n",
    "            targets = batch['targets']\n",
    "            lengths = batch['lengths']\n",
    "            gloss_ids = batch['gloss_ids'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(features, lengths)\n",
    "            \n",
    "            # Compute loss\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            log_probs = log_probs.transpose(0, 1)\n",
    "            \n",
    "            target_lengths = torch.LongTensor([len(t) for t in targets]).to(device)\n",
    "            target_flat = torch.cat(targets).to(device)\n",
    "            \n",
    "            loss = ctc_loss_fn(log_probs, target_flat, lengths, target_lengths)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Greedy decoding (simple accuracy check)\n",
    "            predictions = logits.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "            \n",
    "            # For each sample, get most common non-blank prediction\n",
    "            for i in range(len(gloss_ids)):\n",
    "                pred_seq = predictions[i, :lengths[i]].cpu().numpy()\n",
    "                # Remove blanks and get most common\n",
    "                non_blank = pred_seq[pred_seq != 0]\n",
    "                if len(non_blank) > 0:\n",
    "                    pred_id = np.bincount(non_blank).argmax()\n",
    "                else:\n",
    "                    pred_id = 0  # All blanks\n",
    "                \n",
    "                if pred_id == gloss_ids[i].item():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            \n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    accuracy = (correct / total * 100) if total > 0 else 0.0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"\u2713 Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting Training\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train (returns tuple: total_loss, ctc_loss, blank_penalty)\n",
    "    train_loss, train_ctc_loss, train_blank_penalty = train_epoch(model, train_loader, optimizer, ctc_loss_fn, DEVICE, epoch, BLANK_PENALTY)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, ctc_loss_fn, DEVICE, epoch)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} (CTC: {train_ctc_loss:.4f}, Blank Penalty: {train_blank_penalty:.4f})\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "            'vocab_size': vocab_size,\n",
    "            'gloss_to_id': gloss_to_id,\n",
    "            'id_to_gloss': id_to_gloss,\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'num_layers': NUM_LAYERS,\n",
    "            'dropout': DROPOUT\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, CHECKPOINT_DIR / 'checkpoint_best.pt')\n",
    "        print(f\"  \u2713 Saved best model (val_acc: {val_acc:.2f}%)\")\n",
    "    \n",
    "    # Save periodic checkpoints\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(checkpoint, CHECKPOINT_DIR / f'checkpoint_epoch_{epoch}.pt')\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best Val Accuracy: {best_val_acc:.2f}% (epoch {best_epoch})\")\n",
    "print(f\"Final Val Accuracy: {val_accs[-1]:.2f}%\")\n",
    "print(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoints to Google Drive\n",
    "import shutil\n",
    "\n",
    "drive_checkpoint_dir = Path('/content/drive/MyDrive/asl_data/checkpoints_wlasl100')\n",
    "drive_checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy all checkpoints\n",
    "for checkpoint_file in CHECKPOINT_DIR.glob('*.pt'):\n",
    "    shutil.copy2(checkpoint_file, drive_checkpoint_dir / checkpoint_file.name)\n",
    "    print(f\"\u2713 Copied {checkpoint_file.name}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Results saved to Google Drive\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Checkpoints: {drive_checkpoint_dir}\")\n",
    "print(f\"Best model: {drive_checkpoint_dir / 'checkpoint_best.pt'}\")\n",
    "print(f\"\\nYou can now download these files to your laptop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Why Low Accuracy?\n",
    "\n",
    "The training completed but achieved only 3.57% accuracy (vs 1% random baseline). This suggests the model may be:\n",
    "1. Collapsing to blank predictions\n",
    "2. Using incorrect CTC loss format\n",
    "3. Having issues with target format\n",
    "\n",
    "Let's check what the model is actually predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check what the model is predicting\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample batch\n",
    "    sample_batch = next(iter(val_loader))\n",
    "    features = sample_batch['features'].to(DEVICE)\n",
    "    lengths = sample_batch['lengths']\n",
    "    gloss_ids = sample_batch['gloss_ids']\n",
    "    \n",
    "    # Get predictions\n",
    "    logits = model(features, lengths)\n",
    "    predictions = logits.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "    \n",
    "    # Check blank ratio\n",
    "    blank_count = (predictions == 0).sum().item()\n",
    "    total_predictions = predictions.numel()\n",
    "    blank_ratio = blank_count / total_predictions\n",
    "    \n",
    "    print(f\"Sample batch analysis:\")\n",
    "    print(f\"  Batch size: {len(gloss_ids)}\")\n",
    "    print(f\"  Blank predictions: {blank_count}/{total_predictions} ({blank_ratio*100:.1f}%)\")\n",
    "    print(f\"  True gloss IDs: {gloss_ids.tolist()[:5]}...\")\n",
    "    print(f\"  Predicted (first frame): {predictions[:, 0].cpu().tolist()[:5]}...\")\n",
    "    print(f\"  Most common prediction: {torch.bincount(predictions.flatten().cpu()).argmax().item()}\")\n",
    "    \n",
    "    # Check if model is just predicting one class\n",
    "    unique_preds = torch.unique(predictions)\n",
    "    print(f\"  Unique predictions in batch: {len(unique_preds)} (out of {vocab_size} possible)\")\n",
    "    \n",
    "    if blank_ratio > 0.8:\n",
    "        print(f\"\\n\u26a0 WARNING: Model is predicting mostly blanks (>80%)\")\n",
    "        print(f\"   This suggests blank collapse - the model learned to always predict blank token.\")\n",
    "    elif len(unique_preds) < 5:\n",
    "        print(f\"\\n\u26a0 WARNING: Model is only predicting {len(unique_preds)} different classes\")\n",
    "        print(f\"   This suggests the model collapsed to a few classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix: Improved Training Setup\n",
    "\n",
    "The issues found:\n",
    "- 52% blank predictions (too high)\n",
    "- Only 17/101 classes being predicted\n",
    "- Model needs better regularization and training\n",
    "\n",
    "**Solutions**:\n",
    "1. Increase blank penalty significantly (0.1 \u2192 0.5)\n",
    "2. Use better blank penalty computation (ratio-based, like working code)\n",
    "3. Increase model capacity slightly (hidden_dim 128 \u2192 256)\n",
    "4. Lower learning rate for more stable training\n",
    "5. More aggressive gradient clipping\n",
    "\n",
    "Let's create a fixed training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved training function with better blank penalty\n",
    "def compute_blank_ratio(logits, blank_id=0):\n",
    "    \"\"\"Compute ratio of blank predictions.\"\"\"\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    blank_count = (predictions == blank_id).sum().item()\n",
    "    total_count = predictions.numel()\n",
    "    return blank_count / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_epoch_fixed(model, train_loader, optimizer, ctc_loss_fn, device, epoch, blank_penalty=0.5):\n",
    "    \"\"\"Improved training function with better blank penalty computation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_ctc_loss = 0.0\n",
    "    total_blank_penalty = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
    "        features = batch['features'].to(device)\n",
    "        targets = batch['targets']\n",
    "        lengths = batch['lengths']\n",
    "        gloss_ids = batch['gloss_ids'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(features, lengths)\n",
    "        \n",
    "        # Prepare CTC inputs (PyTorch format: seq_len, batch, vocab)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        log_probs = log_probs.transpose(0, 1)  # (seq_len, batch_size, vocab_size)\n",
    "        \n",
    "        # Targets for CTC\n",
    "        target_lengths = torch.LongTensor([len(t) for t in targets]).to(device)\n",
    "        target_flat = torch.cat(targets).to(device)\n",
    "        \n",
    "        # Compute CTC loss\n",
    "        ctc_loss_value = ctc_loss_fn(log_probs, target_flat, lengths, target_lengths)\n",
    "        \n",
    "        # Better blank penalty: ratio-based (like working code)\n",
    "        blank_ratio = compute_blank_ratio(logits, blank_id=0)\n",
    "        blank_penalty_value = blank_penalty * blank_ratio\n",
    "        \n",
    "        # Total loss\n",
    "        loss = ctc_loss_value + blank_penalty_value\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Increased from 1.0\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_ctc_loss += ctc_loss_value.item()\n",
    "        total_blank_penalty += blank_penalty_value.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / num_batches if num_batches > 0 else 0.0,\n",
    "        'ctc_loss': total_ctc_loss / num_batches if num_batches > 0 else 0.0,\n",
    "        'blank_penalty': total_blank_penalty / num_batches if num_batches > 0 else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\u2713 Improved training function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option: Retrain with Improved Hyperparameters\n",
    "\n",
    "To retrain with better settings, you would:\n",
    "\n",
    "1. **Increase blank penalty**: 0.1 \u2192 0.5 (more aggressive blank suppression)\n",
    "2. **Increase model size**: hidden_dim 128 \u2192 256 (more capacity for 100 classes)\n",
    "3. **Lower learning rate**: 1e-3 \u2192 5e-4 (more stable training)\n",
    "4. **Better blank penalty**: Use ratio-based instead of mean-based\n",
    "\n",
    "**However**, the current model did train and is saved. The 3.57% accuracy suggests:\n",
    "- Model is learning something (better than 1% random)\n",
    "- But not well enough (should be 10-30% for this task)\n",
    "- May need more data per class, better architecture, or longer training\n",
    "\n",
    "**Recommendations**:\n",
    "1. **Keep current checkpoints** - They represent a baseline\n",
    "2. **Try improving** - Use the fixed training function above with better hyperparameters\n",
    "3. **Analyze** - Check per-class accuracy to see which signs work and which don't\n",
    "\n",
    "Would you like to:\n",
    "- **A**: Analyze the current model further (see per-class performance)\n",
    "- **B**: Retrain with improved hyperparameters (will take 1-2 more hours)\n",
    "- **C**: Accept baseline and document findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a0\ufe0f IMPORTANT: Are We Following the Paper?\n",
    "\n",
    "**NO - We're skipping Stage 1 pre-training!**\n",
    "\n",
    "### Paper's 3-Stage Curriculum (Section 7.2):\n",
    "\n",
    "**Stage 1**: Self-supervised phonological pre-training\n",
    "- \u2705 Contrastive learning on unlabeled video (temporal coherence)\n",
    "- \u2705 Phonological quantizer training (Product VQ)\n",
    "- \u2705 Loss: L_contrast + L_phon\n",
    "- \u2705 **Purpose**: Learn robust features BEFORE supervised learning\n",
    "\n",
    "**Stage 2**: End-to-end CTC training (what we're doing)\n",
    "- \u2705 CTC loss for gloss prediction\n",
    "- \u274c Should use **pre-trained encoder from Stage 1**\n",
    "- \u274c Should include multi-task loss: L_CTC + \u03bb_seg L_seg + \u03bb_phon L_phon\n",
    "- \u274c We're training from random initialization!\n",
    "\n",
    "**Stage 3**: WFST fine-tuning (future work)\n",
    "- Discriminative lattice training\n",
    "- Full pipeline with discourse/morphology\n",
    "\n",
    "### What We Actually Did:\n",
    "\n",
    "1. \u274c **Skipped Stage 1** - No pre-training\n",
    "2. \u26a0\ufe0f **Stage 2 only** - Training CTC from scratch\n",
    "3. \u274c **No pre-trained encoder** - Random initialization\n",
    "4. \u274c **Simplified loss** - Only CTC, no multi-task components\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "The paper's approach expects:\n",
    "- **Stage 1** trains the encoder to learn good phonological features\n",
    "- **Stage 2** fine-tunes with pre-trained weights for better initialization\n",
    "- Without Stage 1, we're training a random encoder on limited labeled data\n",
    "\n",
    "**This likely explains the poor performance (3.57%)!**\n",
    "\n",
    "### Solution Options:\n",
    "\n",
    "**Option A**: Run Stage 1 pre-training first (on all WLASL videos, unlabeled)\n",
    "- Learn phonological features\n",
    "- Then run Stage 2 with pre-trained weights\n",
    "- **Expected**: Better performance (10-30% accuracy)\n",
    "\n",
    "**Option B**: Continue with current approach (document as baseline)\n",
    "- Faster to run (skips pre-training)\n",
    "- But limited by random initialization\n",
    "- Useful as \"no pre-training\" baseline\n",
    "\n",
    "**Option C**: Hybrid - Use existing Stage 1 checkpoint (if available)\n",
    "- Check if we have `checkpoints/stage1/checkpoint_best.pt`\n",
    "- Load pre-trained encoder weights\n",
    "- Fine-tune on 100-sign dataset\n",
    "\n",
    "**Recommendation**: Stage 1 pre-training is important per the paper. The poor results likely stem from skipping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 Found Stage 1 Checkpoint!\n",
    "\n",
    "**Location**: `checkpoints/stage1/checkpoint_best.pt`\n",
    "\n",
    "According to STATUS.md:\n",
    "- \u2705 Stage 1 Pre-training complete\n",
    "- Loss: 2.03 (contrastive + VQ)\n",
    "- Pre-trained encoder ready for Stage 2\n",
    "\n",
    "**Next**: Load this checkpoint to initialize the encoder with pre-trained weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option: Load Pre-trained Encoder (Following Paper)\n",
    "\n",
    "To follow the paper's curriculum, we should:\n",
    "1. Upload the Stage 1 checkpoint to Google Drive\n",
    "2. Load the pre-trained encoder weights\n",
    "3. Fine-tune with Stage 2 CTC training\n",
    "\n",
    "This should improve performance significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Stage 1 encoder\n",
    "# First, you'll need to upload checkpoints/stage1/checkpoint_best.pt to Google Drive\n",
    "# Then uncomment and run this:\n",
    "\n",
    "\"\"\"\n",
    "# Path to Stage 1 checkpoint (after uploading to Drive)\n",
    "stage1_checkpoint_path = Path('/content/drive/MyDrive/asl_data/checkpoints/stage1/checkpoint_best.pt')\n",
    "\n",
    "if stage1_checkpoint_path.exists():\n",
    "    print(f\"Loading pre-trained encoder from {stage1_checkpoint_path}...\")\n",
    "    stage1_checkpoint = torch.load(stage1_checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "    \n",
    "    # Check checkpoint structure\n",
    "    print(f\"\\nCheckpoint keys: {list(stage1_checkpoint.keys())}\")\n",
    "    \n",
    "    # Extract encoder weights (Stage 1 checkpoint format)\n",
    "    if 'encoder_state_dict' in stage1_checkpoint:\n",
    "        encoder_state = stage1_checkpoint['encoder_state_dict']\n",
    "    elif 'encoder' in stage1_checkpoint:\n",
    "        encoder_state = stage1_checkpoint['encoder']\n",
    "    else:\n",
    "        # Try to load from model_state_dict if it has encoder key\n",
    "        if 'model_state_dict' in stage1_checkpoint:\n",
    "            model_state = stage1_checkpoint['model_state_dict']\n",
    "            # Extract encoder keys (assuming they start with 'encoder.')\n",
    "            encoder_state = {k.replace('encoder.', ''): v \n",
    "                           for k, v in model_state.items() \n",
    "                           if k.startswith('encoder.')}\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f Checkpoint format not recognized. Available keys:\")\n",
    "            for key in stage1_checkpoint.keys():\n",
    "                print(f\"  - {key}\")\n",
    "            encoder_state = None\n",
    "    \n",
    "    if encoder_state:\n",
    "        # Load encoder weights (matching keys to our encoder)\n",
    "        encoder.load_state_dict(encoder_state, strict=False)\n",
    "        print(f\"\u2713 Loaded pre-trained encoder weights!\")\n",
    "        print(f\"  Stage 1 epoch: {stage1_checkpoint.get('epoch', 'unknown')}\")\n",
    "        print(f\"  Stage 1 loss: {stage1_checkpoint.get('best_loss', stage1_checkpoint.get('loss', 'unknown')):.4f}\")\n",
    "        print(f\"\\n  \u26a0\ufe0f Note: Using 'strict=False' because vocab size may differ\")\n",
    "        print(f\"  (Stage 1 was on 20 signs, we're training on 100 signs)\")\n",
    "        print(f\"\\n  \u2713 Encoder initialized with pre-trained features from Stage 1\")\n",
    "        print(f\"  \u2713 Now ready for Stage 2 CTC fine-tuning (following paper!)\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f Could not extract encoder weights from checkpoint\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Stage 1 checkpoint not found at {stage1_checkpoint_path}\")\n",
    "    print(f\"   Please upload checkpoints/stage1/checkpoint_best.pt to Google Drive first\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\u2713 Pre-trained encoder loading code ready\")\n",
    "print(\"\\n\ud83d\udccb Instructions:\")\n",
    "print(\"1. Upload checkpoints/stage1/checkpoint_best.pt from your laptop to Google Drive\")\n",
    "print(\"2. Place it at: /content/drive/MyDrive/asl_data/checkpoints/stage1/checkpoint_best.pt\")\n",
    "print(\"3. Uncomment and run the code above to load pre-trained weights\")\n",
    "print(\"4. Then retrain with the pre-trained encoder!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}